{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\rober\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rober\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rober\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.133.253:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [12/Jun/2024 14:00:16] \"POST /summarize HTTP/1.1\" 400 -\n",
      "127.0.0.1 - - [12/Jun/2024 14:00:39] \"POST /summarize HTTP/1.1\" 400 -\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "127.0.0.1 - - [12/Jun/2024 14:01:01] \"POST /summarize HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jun/2024 14:06:34] \"POST /summarize HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Jun/2024 11:27:35] \"POST /summarize HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import textwrap\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "summarizer = pipeline('summarization')\n",
    "\n",
    "def summarize_article(url):\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    results = soup.find_all(['h1', 'p'])\n",
    "    text = (result.text for result in results)\n",
    "    article = ' '.join(text)\n",
    "    article = article.replace('.', '.<eos>').replace('?', '?<eos>').replace('!', '!<eos>')\n",
    "    sentences = article.split('<eos>')\n",
    "\n",
    "    max_chunk = 450\n",
    "    current_chunk = 0\n",
    "    chunks = []\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1:\n",
    "            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk].extend(sentence.split(' '))\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split(' '))\n",
    "        else:\n",
    "            chunks.append(sentence.split(' '))\n",
    "\n",
    "    for chunk_id in range(len(chunks)):\n",
    "        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=40, min_length=10, do_sample=False)\n",
    "        summaries.append(summary[0]['summary_text'])\n",
    "\n",
    "    final_summary = ' '.join(summaries)\n",
    "    wrapped_summary = textwrap.fill(final_summary, width=80)\n",
    "    return wrapped_summary\n",
    "\n",
    "@app.route('/summarize', methods=['POST'])\n",
    "def summarize():\n",
    "    data = request.get_json()\n",
    "    url = data['url']\n",
    "    summary = summarize_article(url)\n",
    "    return jsonify({'summary': summary})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
